{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4232754a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc151cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebfa4350",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c3a73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and Extract text from PDF files\n",
    "def load_pdf_files(data):           # Function set-up to load PDF files from a directory\n",
    "    loader = DirectoryLoader(       # Load all PDF files from the specified directory\n",
    "        data,\n",
    "        glob=\"*.pdf\",               # Only consider files with .pdf extension\n",
    "        loader_cls=PyPDFLoader      # Use PyPDFLoader loader class to handle PDF files\n",
    "    )\n",
    "\n",
    "    documents = loader.load()       # Load the documents from the directory, it returns a list of document objects (each document object has page_content and metadata of each page)\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cacf0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_data = load_pdf_files(\"data\")  # Load and Extract PDF files from the 'data' directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8585ea15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from langchain.schema import Document\n",
    "\n",
    "def filter_to_minimal_docs(docs: List[Document]) -> List[Document]:         # Function to filter docs. docs is a list of Document objects (extracted_data)\n",
    "    \"\"\"\n",
    "    Given a list of Document objects, return a new list of Document objects\n",
    "    containing only 'source' in metadata and the original page_content.\n",
    "    \"\"\"\n",
    "    minimal_docs: List[Document] = []               # Initialize an empty list to hold the minimal documents, here we use hinting for type safety/production grade code\n",
    "    for doc in docs:\n",
    "         minimal_docs.append(                        # Append only page_content and source metadata\n",
    "            Document(                               # Create a new Document object\n",
    "                page_content=doc.page_content,\n",
    "                metadata={\"source\": doc.metadata.get(\"source\")}\n",
    "            )\n",
    "        )\n",
    "    return minimal_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a049f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "minimal_docs = filter_to_minimal_docs(extracted_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a8ff78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the documents into smaller chunks\n",
    "def text_split(minimal_docs):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(             # Configure the text splitter\n",
    "        chunk_size=500,\n",
    "        chunk_overlap=20                                        # Overlap between chunks to maintain context\n",
    "    )\n",
    "    texts_chunk = text_splitter.split_documents(minimal_docs)   # Call the splitter on the minimal documents\n",
    "    return texts_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc7c869",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_chunk = text_split(minimal_docs)\n",
    "print(f\"Number of chunks: {len(texts_chunk)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ddd5ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8354c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the end of one chunk and the start of the next to verify overlap\n",
    "\n",
    "for i in range(6,8):\n",
    "    print(f\"\\nChunk {i+1} end:\\n{texts_chunk[i].page_content[-50:]}\")\n",
    "    print(f\"Chunk {i+2} start:\\n{texts_chunk[i+1].page_content[:50]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ed5d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download HuggingFace Embeddings Model\n",
    "\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "def download_embeddings():\n",
    "    \"\"\"\n",
    "    Download and return the HuggingFace embeddings model.\n",
    "    \"\"\"\n",
    "    model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=model_name\n",
    "    )\n",
    "    return embeddings\n",
    "\n",
    "embedding = download_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671e0570",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df2a4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241584b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get API keys from environment variables\n",
    "\n",
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "LLAMA_API_KEY = os.getenv(\"LLAMA_API_KEY\")\n",
    "\n",
    "# Set API keys in environment variables\n",
    "\n",
    "os.environ[\"PINECONE_API_KEY\"] = PINECONE_API_KEY  \n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
    "os.environ[\"LLAMA_API_KEY\"] = LLAMA_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7276bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Pinecone client with the API key\n",
    "\n",
    "from pinecone import Pinecone \n",
    "pinecone_api_key = PINECONE_API_KEY\n",
    "\n",
    "pc = Pinecone(api_key=pinecone_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97626b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Pinecone index if it doesn't exist to store the embeddings\n",
    "\n",
    "from pinecone import ServerlessSpec \n",
    "\n",
    "index_name = \"medical-chatbot\"\n",
    "\n",
    "if not pc.has_index(index_name):\n",
    "    pc.create_index(\n",
    "        name = index_name,  \n",
    "        dimension=384,      # Dimension of the embeddings that will be stored in the index\n",
    "        metric= \"cosine\",   # Cosine technique for calculating the similarity\n",
    "        spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\")\n",
    "    )\n",
    "\n",
    "\n",
    "index = pc.Index(index_name)    # Establish connection with the Pinecone index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae49b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Pinecone Vector Store using LangChain for handling document embeddings \n",
    "\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "\n",
    "docsearch = PineconeVectorStore.from_documents(     # Embed each chunk and upsert the embeddings into the Pinecone index\n",
    "    documents=texts_chunk,\n",
    "    embedding=embedding,\n",
    "    index_name=index_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b28080",
   "metadata": {},
   "outputs": [],
   "source": [
    "docsearch = PineconeVectorStore.from_existing_index(    # Use this for connecting to the existing Pinecone index and retrieve the embeddings\n",
    "    index_name=index_name,\n",
    "    embedding=embedding\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2fdc76e",
   "metadata": {},
   "source": [
    "# Add more data to the existing Pinecone index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc313313",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creature a sample Document object\n",
    "\n",
    "dswith = Document(\n",
    "    page_content=\"this is a sampple text chunk to test adding a document object in the pinecone index.\",\n",
    "    metadata={\"source\": \"VSCode\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1046d323",
   "metadata": {},
   "outputs": [],
   "source": [
    "docsearch.add_documents(documents=[dswith])     # Add the sample document to the Pinecone index using the docsearch vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f56976",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a retriever object from the docsearch vector store...\n",
    "# ...to retrieve the top 3 most similar documents based on cosine similarity\n",
    "\n",
    "retriever = docsearch.as_retriever(search_type=\"similarity\", search_kwargs={\"k\":3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c7c64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_docs = retriever.invoke(\"What is Acne?\")      # Use the retriever to find the most relevant documents... \n",
    "retrieved_docs                                          # ...based on the query \"What is Acne?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b05732c",
   "metadata": {},
   "source": [
    "project steps continued"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c053668f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "# from langchain_llama import ChatLlama\n",
    "\n",
    "chatModel = ChatOpenAI(model=\"gpt-4o\")\n",
    "# chatModel = ChatLlama(model=\"llama-2-70b-chat-hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd20257d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d5b331",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = (\n",
    "    \"You are an Medical assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Use three sentences maximum and keep the \"\n",
    "    \"answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "\"\"\"\n",
    "    The below prompt template structure makes it multi-turn.\n",
    "    Each message is treated as part of a conversation history, not a signle prompt.\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec49f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the RAG chain using the retriever and the LLM model\n",
    "\n",
    "question_answer_chain = create_stuff_documents_chain(chatModel, prompt)     # Create a chain that combines the retrieved documents and the LLM to generate an answer based on the context and the question\n",
    "rag_chain = create_retrieval_chain(retriever, question_answer_chain)        # Create a retrieval chain that first retrieves relevant documents using the retriever and then passes them to the question_answer_chain\n",
    "# In above line docs retrieved by the retriever are automatically passed to the create_stuff_documents_chain as context. We don't have to explicitly pass them in the prompt template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ddec90",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = rag_chain.invoke({\"input\": \"what is Acromegaly and gigantism?\"})\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d073e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = rag_chain.invoke({\"input\": \"is there are any sample text?\"})\n",
    "print(response[\"answer\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medibot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
